\subsection{Вычислительная сложность операций метода эллипсоидов}

В индустрии разработки программного обеспечения известен феномен, который состоит в том, что на 20\% методов программы приходится 80\% времени ее выполнения~\cite{Boehm1987}. Такое эмпирическое правило хорошо согласуется с более общим принципом, известным как принцип Парето либо <<правило 80/20>>.
\begin{statement}[Принцип Парето]
80\% результата можно получить, приложив 20\% усилий.
\end{statement}
Относящийся не только к программированию, этот принцип очень точно характеризует оптимизацию программ~\cite{McConnell2010}. В работе~\cite{Knuth1971} Дональд Кнут указал, что менее 4\% кода обычно соответствуют более чем 50\% времени выполнения программы. Опираясь на принцип Парето, можно сформулировать последовательность действий, приводящих к ускорению работы имеющихся программ: необходимо найти в коде <<горячие точки>> и сосредоточиться на оптимизации наиболее трудоемких процессов.

Проанализируем подробнее вычислительную сложность операций алгоритма метода эллипсоидов. Для этого введем некоторые обозначения из области асимптотического анализа~\cite{Grin_Knuth1987}.

\begin{definition}
\label{def:O}
Функция $f(n)$ \emph{ограничена сверху} функцией $g(n)$ асимптотически с точностью до постоянного множителя, т.е. $$f(n)=O\left(g(n)\right),$$ если существуют целые $N$ и $K$, такие, что $|f(n)|\le Kg(n)$ при всех $n\ge N$.
\end{definition}

\begin{definition}
\label{def:Omega}
Функция $f(n)$ \emph{ограничена снизу} функцией $g(n)$ асимптотически с точностью до постоянного множителя, т.е. $$f(n)=\Omega\left(g(n)\right),$$ если существуют целые $N$ и $K$, такие, что $f(n)\ge Kg(n)$ при всех $n\ge N$.
\end{definition}

\begin{definition}
\label{def:Theta}
Функция $f(n)$ \emph{ограничена снизу и сверху} функцией $g(n)$ асимптотически с точностью до постоянного множителя, т.е.$$f(n)=\Theta\left(g(n)\right),$$ если одновременно выполнены условия определений~\ref{def:O} и~\ref{def:Omega}.
\end{definition}

На \textbf{шаге 1} алгоритма~\ref{alg:EllipsoidMethod} изменение текущего субградиента $g(x_k)$ происходит на основании анализа значений функций ограничений~(\ref{eq:limits}) в текущей точке $x_k$, т.е. анализируется последовательность значений $$\max_{1\le i\le m}f_i(x_k).$$ Вычислительная сложность поиска максимального из $m$ чисел зависит от выбора алгоритма.

При использовании линейного последовательного поиска цикл выполнит $m$ итераций. Трудоемкость каждой итерации не зависит от количества элементов, поэтому имеет сложность $T^{iter}=O(1)$. В связи с этим, верхняя оценка всего алгоритма поиска $T_m^{min}=O(m)\cdot O(1)=O(m\cdot 1)=O(m)$. Аналогично вычисляется нижняя оценка сложности, а в силу того, что она совпадает с верхней, можно утверждать $T_m^{min}=\Theta(m)$.

При использовании алгоритма бинарного поиска на каждом шаге количество рассматриваемых элементов сокращается в 2 раза. Количество элементов, среди которых может находиться искомый, на $k$-ом шаге определяется формулой $\frac{m}{2^k}$. В худшем случае поиск будет продолжаться, пока в массиве не останется один элемент, т.е. алгоритм имеет логарифмическую сложность: $T_m^{binSearch}=O\left(\log(m)\right)$. Резюмируем все вышесказанное относительно алгоритмов поиска в виде таблицы~\ref{tab:findComplexity}.

\begin{table}[h]
\caption{\label{tab:findComplexity} Оценка сложности некоторых алгоритмов поиска}
\begin{tabular}{ | m{0.15\textwidth} | m{0.3\textwidth} | m{0.15\textwidth} | m{0.15\textwidth} | m{0.15\textwidth} | @{}m{0cm}@{} }\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Алгоритм}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Структура данных}}} & \multicolumn{2}{c|}{\textbf{Временная сложность}}                       & \textbf{Сложность по памяти} & \\ \cline{3-5} 
\multicolumn{1}{|c|}{}                                   & \multicolumn{1}{c|}{}                                           & \centering\textbf{В среднем}       & \centering\textbf{В худшем}        & \centering\textbf{В худшем}  & \\ \hline
Линейный поиск                                           & Массив из $n$ элементов                                         & \centering $O(n)$                  & \centering $O(n)$                  & \centering $O(1)$            & \\ \hline
Бинарный поиск                                           & Отсортированный массив из $n$ элементов                         & \centering $O\left(\log(n)\right)$ & \centering $O\left(\log(n)\right)$ & \centering $O(1)$            & \\ \hline
\end{tabular}
\end{table}

Однако при использовании некоторых алгоритмов (например, алгоритма бинарного поиска) потребуются дополнительные процедуры для упорядочивания входной последовательности значений. В таблице~\ref{tab:sortComplexity} представлены асимптотические оценки наиболее известных алгоритмов сортировки массива из $n$ элементов\footnote{Более подробный анализ приведен в~\cite{Virt2010}.}.

\begin{table}[h]
\caption{\label{tab:sortComplexity} Оценка сложности некоторых алгоритмов сортировки$^1$}
\begin{tabular}{ | m{0.3\textwidth} | m{0.15\textwidth} | m{0.15\textwidth} | m{0.15\textwidth} | m{0.15\textwidth} | @{}m{0cm}@{} }\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Алгоритм}}} & \multicolumn{3}{c|}{\textbf{Временная сложность}}                         & \textbf{Сложность по памяти} & \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                                   & \centering\textbf{В лучшем}         & \centering\textbf{В среднем}        & \centering\textbf{В худшем}         & \centering\textbf{В худшем}  & \\ \hline
Быстрая сортировка                                       & \centering $O\left(n\log(n)\right)$ & \centering $O\left(n\log(n)\right)$ & \centering $O(n^2)$                 & \centering $O(n)$            & \\ \hline
Сортировка слиянием                                      & \centering $O\left(n\log(n)\right)$ & \centering $O\left(n\log(n)\right)$ & \centering $O\left(n\log(n)\right)$ & \centering $O(n)$            & \\ \hline
Пирамидальная сортировка                                 & \centering $O\left(n\log(n)\right)$ & \centering $O\left(n\log(n)\right)$ & \centering $O\left(n\log(n)\right)$ & \centering $O(1)$            & \\ \hline
Пузырьковая сортировка                                   & \centering $O(n)$                   & \centering $O(n^2)$                 & \centering $O(n^2)$                 & \centering $O(1)$            & \\ \hline
Сортировка вставками                                     & \centering $O(n)$                   & \centering $O(n^2)$                 & \centering $O(n^2)$                 & \centering $O(1)$            & \\ \hline
Сортировка выборомм                                      & \centering $O(n^2)$                 & \centering $O(n^2)$                 & \centering $O(n^2)$                 & \centering $O(1)$            & \\ \hline
Блочная сортировка                                       & \centering $O(n+k)$                 & \centering $O(n+k)$                 & \centering $O(n^2)$                 & \centering $O(nk)$           & \\ \hline
Поразрядная сортировка                                   & \centering $O(nk)$                  & \centering $O(nk)$                  & \centering $O(nk)$                  & \centering $O(n+k)$          & \\ \hline
\end{tabular}
\end{table}

На \textbf{шаге 2} алгоритма производится вычисление нормированного обобщенного градиента $$\xi_k=\frac{B_k^Tg(x_k)}{||B_k^Tg(x_k)||},$$ что подразумевает выполнение трудоемких матричных операций, таких как транспонирование, умножение на вектор и на число.  Асимптотическая сложность таких операций для матрицы размерности $n\times n$ может быть оценена как $O(n^2)$.

На \textbf{шаге 3} при обновлении значения текущей точки выполняется умножение матрицы $B_k$ на вектор $\xi_k$: $$x_{k+1}=x_k-h_k\cdot B_k\cdot\xi_k.$$ Аналогично предыдущей оценке, цена такой операции составит $O(n^2)$.

\textbf{Шаг 4} наиболее сложен из-за необходимости вычисления оператора растяжения пространства $$B_{k+1}=B_k\cdot R_\beta(\xi_k).$$ Если представить оператор в матричной форме (определение~\ref{def:operator}), то можно видеть, что в процессе его вычисления используются все вышеперечисленные операции, а также операция сложения матриц, сложность которой составляет $O(n^2)$.

На \textbf{шаге 5} осуществляется пересчет коэффициента $h_k$, отвечающего за уменьшение объема шара $$h_{k+1}=h_k\cdot r.$$ Эта операция может быть выполнена за константное время, т.е. асимптотически ее сложность составит $O(1)$.

Из проведенного анализа вычислительной сложности операций, входящих в алгоритм метода эллипсоидов, можно сделать несколько выводов. Во-первых, учитывая специфику рассматриваемого класса задач (задачи оптимизации большой размерности), наиболее трудоемкие операции будут выполняться значительно дольше менее трудоемких, что приведет к сильно неравномерной загрузке вычислительной системы. Во-вторых, схема чередования сложных/простых в вычислительном смысле операций, а также выбор целевой платформы (многопроцессорные и/или многоядерные системы с общей разделяемой памятью) наталкивают на возможность использования Fork-Join Model (FJM) модели распараллеливания задач для ускорения работы алгоритма метода эллипсоидов.

На основании полученных данных можно сформулировать гипотезу о том, насколько удастся ускорить выполнение метода в целом для решения задач оптимизации большой размерности, если к наиболее ресурсоемким операциям применить алгоритм распараллеливания по данным.

\begin{hypothesis}[О соотношении времени]
Пусть $f(t)$ -- это время работы алгоритма метода эллипсоидов для задачи оптимизации размерности $N\times M$, выполняемого \textit{в однопоточном режиме}. Тогда для параллельной реализации метода эллипсоидов, выполняемой \textit{в многопоточном режиме}, для достаточно больших $N$ и $M$ справедливо равенство $$F(t)=kf(t),$$ где $F(t)$ -- общее время работы параллельной реализации метода, а $k$ -- коэффициент ускорения $(k>1)$.
\end{hypothesis}

Приближенная оценка для коэффициента ускорения $k$ может быть получена в ходе выполнения вычислительных экспериментов.