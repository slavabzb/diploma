В настоящее время большое внимание уделяется созданию автоматизированных систем планирования, проектирования и управления в различных областях промышленности. На первый план выдвигаются вопросы качества принимаемых решений, в связи с чем возрастает роль методов и алгоритмов решения оптимизационных задач в математическом обеспечении автоматизированных систем различного уровня и назначения.

Имеется несколько основных источников, порождающих задачи оптимизации: задачи математического программирования, задачи нелинейного программирования, задачи оптимального управления, задачи дискретного программирования или задачи смешанного дискретно-непрерывного типа~\cite{Shor1979}.

Сфера применения методов оптимизации огромна. Создание эффективных методов оптимизации является ключом к решению многих вычислительных проблем математического программирования, особенно для задач большой размерности.

Для минимизации гладких функций широко применяются различные модификации градиентных процессов, поскольку направление антиградиента в данной точке локально является направлением наискорейшего спуска. Регулировка шага в большинстве алгоритмов этого типа основана на том, чтобы обеспечить монотонное и в достаточной степени <<существенное>> уменьшение значения функции на каждом шаге.

Простейший обобщенный градиентный метод состоит в движении на каждом шаге в направлении, обратном направлению обобщенного градиента. Этот метод под названием обобщенного градиентного спуска (ОГС) предложен Н.З.~Шором в 1961 г. в связи с необходимостью разработки эффективного алгоритма решения транспортных задач большой размерности для задач текущего планирования, решаемых в Институте кибернетики АН УССР совместно с Госпланом УССР. Впервые метод обобщенного градиентного спуска для минимизации кусочно-линейных выпуклых функций использовался при решении транспортных и транспортно-производственных задач~\cite{Shor1962}. Затем метод ОГС был распространен на класс произвольных выпуклых функций~\cite{Shor1964} и на задачи выпуклого программирования в гильбертовом пространстве~\cite{Polyak1967}. Широкое распространение получили стохастические аналоги ОГС~\cite{Ermolyev1976}. Алгоритмы решения задач математического программирования, построенные на основе ОГС, отличаются простотой и, что особенно важно для задач большой размерности, экономным использованием оперативной памяти ЭВМ.

К ограничениям метода ОГС относятся его довольно медленная сходимость, сложность контроля точности решения и то, что он применим только к классу выпуклых функций.

В 1969--1970 гг. Н.З.~Шором были предложены ускоренные варианты обобщенных градиентных методов, основанные на использовании операции растяжения пространства в направлении градиента и разнсти двух последовательных градиентов~\cite{Shor1970}. Идея этих методов существенно отлична от той, которая используется для ускорения сходимости в случае гладких функций -- идеи квадратичной аппроксимации функции в окрестности минимума, в той или иной мере определяющей формализм как методов сопряженных градиентов, так и квазиньютоновских методов~\cite{Huang1970}. В то же время предельные варианты методов с растяжением пространства при определенных условиях регулярности и гладкости обладают свойством квадратичной скорости сходимости. Таким образом, предложенные алгоритмы обладают высокой эффективностью и применительно к гладким задачам минимизации. В дальнейшем алгоритмы с растяжением пространства были обобщены на задачи нахождения локальных минимумов невыпуклых негладких функций~\cite{Shor1972}.

В США и Западной Европе градиентными методами минимизации негладких функций всерьез начали заниматься примерно с 1973 г. сначала в связи с приложениями в области дискретного программирования~\cite{Held_Karp1971}, а затем в целом для решения задач большой размерности~\cite{Held_Wolfe_Crowder1974}. Результаты работ в этом направлении на Западе достаточно полно представлены в сборнике~\cite{Balinski_Wolfe1975}. Особенно интенсивно развивается направление так называемой $\varepsilon$-субградиентной оптимизации, по идее близкое, с одной стороны, к алгоритмам В.Ф.~Демьянова решения минимаксных задач, а с другой, особенно в формальном отношении, -- к алгоритмам метода сопряженных градиентов (или <<давидоновского>> типа).

И наконец, в последнее время обнаружились очень интересные связи между алгоритмами последовательных отсечений и алгоритмами с растяжением пространства~\cite{Shor1977,Yudin_Nemirovsky1976}.

Таким образом, область обобщенных градиентных методов оптимизации не представляет нечто окончательно сформировавшееся и застывшее, а, наоборот, быстро развивается.